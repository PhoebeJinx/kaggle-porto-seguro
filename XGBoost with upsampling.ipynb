{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T08:08:54.855742Z",
     "start_time": "2017-11-05T23:08:09.170404-08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.1Fold  1 : 0.295349 @ 200 / best score is 0.295923 @ 199\n",
      "Fold  2 : 0.280354 @ 200 / best score is 0.280510 @ 193\n",
      "Fold  3 : 0.287463 @ 200 / best score is 0.287924 @ 199\n",
      "Fold  4 : 0.299788 @ 200 / best score is 0.300325 @ 199\n",
      "Fold  5 : 0.261463 @ 200 / best score is 0.261856 @ 188\n",
      "Fold  6 : 0.283628 @ 200 / best score is 0.283789 @ 187\n",
      "Fold  7 : 0.290999 @ 200 / best score is 0.291114 @ 185\n",
      "Fold  8 : 0.287632 @ 200 / best score is 0.287935 @  88\n",
      "Fold  9 : 0.285866 @ 200 / best score is 0.286057 @ 108\n",
      "Fold 10 : 0.278732 @ 200 / best score is 0.279081 @ 127\n",
      "Fold 11 : 0.320424 @ 200 / best score is 0.320606 @ 199\n",
      "Fold 12 : 0.267349 @ 200 / best score is 0.267593 @ 164\n",
      "Fold 13 : 0.262811 @ 200 / best score is 0.262926 @ 195\n",
      "Fold 14 : 0.296846 @ 200 / best score is 0.297580 @ 105\n",
      "Fold 15 : 0.301287 @ 200 / best score is 0.301960 @ 105\n",
      "Fold 16 : 0.286563 @ 200 / best score is 0.286893 @ 168\n",
      "Fold 17 : 0.273377 @ 200 / best score is 0.273488 @ 149\n",
      "Fold 18 : 0.283549 @ 200 / best score is 0.283943 @ 142\n",
      "Fold 19 : 0.289547 @ 200 / best score is 0.289795 @ 139\n",
      "Fold 20 : 0.304071 @ 200 / best score is 0.304649 @ 139\n",
      "Full OOF score : 0.286740\n",
      "Best mean score : 0.286054 + 0.014199 @ 182\n",
      "ps_car_13                          :     0.1185\n",
      "ps_reg_03                          :     0.0857\n",
      "ps_ind_03                          :     0.0660\n",
      "ps_ind_15                          :     0.0460\n",
      "ps_car_14                          :     0.0451\n",
      "ps_ind_01                          :     0.0395\n",
      "ps_reg_02                          :     0.0378\n",
      "ps_reg_01_plus_ps_car_04_cat_avg   :     0.0362\n",
      "ps_car_11_cat_avg                  :     0.0354\n",
      "ps_ind_05_cat_avg                  :     0.0306\n",
      "ps_car_01_cat_avg                  :     0.0277\n",
      "ps_car_15                          :     0.0267\n",
      "ps_car_11_cat                      :     0.0264\n",
      "ps_reg_01_plus_ps_car_02_cat_avg   :     0.0255\n",
      "ps_car_12                          :     0.0232\n",
      "ps_ind_17_bin                      :     0.0230\n",
      "ps_car_09_cat_avg                  :     0.0189\n",
      "ps_calc_05                         :     0.0184\n",
      "ps_car_01_cat                      :     0.0177\n",
      "ps_reg_01_plus_ps_car_04_cat       :     0.0165\n",
      "ps_car_06_cat                      :     0.0161\n",
      "ps_calc_09                         :     0.0157\n",
      "ps_ind_02_cat_avg                  :     0.0154\n",
      "ps_car_07_cat                      :     0.0149\n",
      "ps_car_06_cat_avg                  :     0.0138\n",
      "ps_car_03_cat                      :     0.0128\n",
      "ps_ind_05_cat                      :     0.0109\n",
      "ps_reg_01_plus_ps_car_02_cat       :     0.0106\n",
      "ps_ind_07_bin                      :     0.0102\n",
      "ps_car_11                          :     0.0102\n",
      "ps_car_04_cat_avg                  :     0.0101\n",
      "ps_ind_02_cat                      :     0.0098\n",
      "ps_ind_16_bin                      :     0.0087\n",
      "ps_car_09_cat                      :     0.0084\n",
      "ps_ind_06_bin                      :     0.0083\n",
      "ps_ind_09_bin                      :     0.0079\n",
      "ps_ind_04_cat                      :     0.0074\n",
      "ps_car_05_cat                      :     0.0059\n",
      "ps_ind_08_bin                      :     0.0059\n",
      "ps_reg_01                          :     0.0056\n",
      "ps_ind_04_cat_avg                  :     0.0045\n",
      "ps_car_04_cat                      :     0.0041\n",
      "ps_car_08_cat                      :     0.0041\n",
      "ps_ind_18_bin                      :     0.0030\n",
      "ps_car_07_cat_avg                  :     0.0026\n",
      "ps_ind_14                          :     0.0019\n",
      "ps_car_03_cat_avg                  :     0.0017\n",
      "ps_ind_12_bin                      :     0.0015\n",
      "ps_car_05_cat_avg                  :     0.0011\n",
      "ps_car_02_cat                      :     0.0009\n",
      "ps_car_08_cat_avg                  :     0.0007\n",
      "ps_car_02_cat_avg                  :     0.0003\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\"\"\"\n",
    "This simple scripts demonstrates the use of xgboost eval results to get the best round\n",
    "for the current fold and accross folds. \n",
    "It also shows an upsampling method that limits cross-validation overfitting.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "from numba import jit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time \n",
    "\n",
    "\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Original author CPMP : https://www.kaggle.com/cpmpml\n",
    "    In kernel : https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]\n",
    "\n",
    "\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "trn_df = pd.read_csv(\"../data/train.csv\", index_col=0)\n",
    "sub_df = pd.read_csv(\"../data/test.csv\", index_col=0)\n",
    "\n",
    "target = trn_df[\"target\"]\n",
    "del trn_df[\"target\"]\n",
    "\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\t\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\t\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\t\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\t\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\t\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\t\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\t\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\t\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\t\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\t\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "\t\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\t\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\t\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\t\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\t\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "\t\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "\t\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\t\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\t\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "\t\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\t\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\t\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\t\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\t\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\t\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "\t\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "\t\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "\t\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\t\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "\t\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\t\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "\t\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\t\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "\t\"ps_car_11_cat\" # Very nice spot from Tilii : https://www.kaggle.com/tilii7\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "start = time.time()\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    trn_df[name1] = trn_df[f1].apply(lambda x: str(x)) + \"_\" + trn_df[f2].apply(lambda x: str(x))\n",
    "    sub_df[name1] = sub_df[f1].apply(lambda x: str(x)) + \"_\" + sub_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(trn_df[name1].values) + list(sub_df[name1].values))\n",
    "    trn_df[name1] = lbl.transform(list(trn_df[name1].values))\n",
    "    sub_df[name1] = lbl.transform(list(sub_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "trn_df = trn_df[train_features]\n",
    "sub_df = sub_df[train_features]\n",
    "\n",
    "f_cats = [f for f in trn_df.columns if \"_cat\" in f]\n",
    "\n",
    "for f in f_cats:\n",
    "    trn_df[f + \"_avg\"], sub_df[f + \"_avg\"] = target_encode(trn_series=trn_df[f],\n",
    "                                         tst_series=sub_df[f],\n",
    "                                         target=target,\n",
    "                                         min_samples_leaf=200,\n",
    "                                         smoothing=10,\n",
    "                                         noise_level=0)\n",
    "\n",
    "n_splits = 20\n",
    "n_estimators = 200\n",
    "folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=15) \n",
    "imp_df = np.zeros((len(trn_df.columns), n_splits))\n",
    "xgb_evals = np.zeros((n_estimators, n_splits))\n",
    "oof = np.empty(len(trn_df))\n",
    "sub_preds = np.zeros(len(sub_df))\n",
    "increase = True\n",
    "np.random.seed(0)\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(target, target)):\n",
    "    trn_dat, trn_tgt = trn_df.iloc[trn_idx], target.iloc[trn_idx]\n",
    "    val_dat, val_tgt = trn_df.iloc[val_idx], target.iloc[val_idx]\n",
    "\n",
    "    clf = XGBClassifier(n_estimators=n_estimators,\n",
    "                        max_depth=4,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=.1, \n",
    "                        subsample=.8, \n",
    "                        colsample_bytree=.8,\n",
    "                        gamma=1,\n",
    "                        reg_alpha=0,\n",
    "                        reg_lambda=1,\n",
    "                        nthread=2)\n",
    "    # Upsample during cross validation to avoid having the same samples\n",
    "    # in both train and validation sets\n",
    "    # Validation set is not up-sampled to monitor overfitting\n",
    "    if increase:\n",
    "        # Get positive examples\n",
    "        pos = pd.Series(trn_tgt == 1)\n",
    "        # Add positive examples\n",
    "        trn_dat = pd.concat([trn_dat, trn_dat.loc[pos]], axis=0)\n",
    "        trn_tgt = pd.concat([trn_tgt, trn_tgt.loc[pos]], axis=0)\n",
    "        # Shuffle data\n",
    "        idx = np.arange(len(trn_dat))\n",
    "        np.random.shuffle(idx)\n",
    "        trn_dat = trn_dat.iloc[idx]\n",
    "        trn_tgt = trn_tgt.iloc[idx]\n",
    "        \n",
    "    clf.fit(trn_dat, trn_tgt, \n",
    "            eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n",
    "            eval_metric=gini_xgb,\n",
    "            early_stopping_rounds=None,\n",
    "            verbose=False)\n",
    "            \n",
    "    # Keep feature importances\n",
    "    imp_df[:, fold_] = clf.feature_importances_\n",
    "\n",
    "    # Find best round for validation set\n",
    "    xgb_evals[:, fold_] = clf.evals_result_[\"validation_1\"][\"gini\"]\n",
    "    # Xgboost provides best round starting from 0 so it has to be incremented\n",
    "    #best_round = np.argsort(xgb_evals[:, fold_])[::-1][0]\n",
    "    best_round = int(np.argsort(xgb_evals[:, fold_])[::-1][0])\n",
    "\n",
    "    # Predict OOF and submission probas with the best round\n",
    "    oof[val_idx] = clf.predict_proba(val_dat, ntree_limit=best_round)[:, 1]\n",
    "    # Update submission\n",
    "    sub_preds += clf.predict_proba(sub_df, ntree_limit=best_round)[:, 1] / n_splits\n",
    "\n",
    "    # Display results\n",
    "    print(\"Fold %2d : %.6f @%4d / best score is %.6f @%4d\"\n",
    "          % (fold_ + 1,\n",
    "             eval_gini(val_tgt, oof[val_idx]),\n",
    "             n_estimators,\n",
    "             xgb_evals[best_round, fold_],\n",
    "             best_round))\n",
    "          \n",
    "print(\"Full OOF score : %.6f\" % eval_gini(target, oof))\n",
    "\n",
    "# Compute mean score and std\n",
    "mean_eval = np.mean(xgb_evals, axis=1)\n",
    "std_eval = np.std(xgb_evals, axis=1)\n",
    "best_round = np.argsort(mean_eval)[::-1][0]\n",
    "\n",
    "print(\"Best mean score : %.6f + %.6f @%4d\"\n",
    "      % (mean_eval[best_round], std_eval[best_round], best_round))\n",
    "    \n",
    "importances = sorted([(trn_df.columns[i], imp) for i, imp in enumerate(imp_df.mean(axis=1))],\n",
    "                     key=lambda x: x[1])\n",
    "\n",
    "for f, imp in importances[::-1]:\n",
    "    print(\"%-34s : %10.4f\" % (f, imp))\n",
    "    \n",
    "sub_df[\"target\"] = sub_preds\n",
    "\n",
    "sub_df[[\"target\"]].to_csv(\"submission.csv\", index=True, float_format=\"%.9f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
